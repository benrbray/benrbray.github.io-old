<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Benjamin R. Bray - Mathematics</title><link href="http://benrbray.com/" rel="alternate"></link><link href="http://localhost:8000/feeds/mathematics.atom.xml" rel="self"></link><id>http://benrbray.com/</id><updated>2018-05-02T00:00:00-04:00</updated><entry><title>Algorithms for Random Discrete Structures</title><link href="http://benrbray.com/posts/2018/algorithms-for-random-discrete-structures" rel="alternate"></link><published>2018-05-02T00:00:00-04:00</published><updated>2018-05-02T00:00:00-04:00</updated><author><name>Benjamin R. Bray</name></author><id>tag:benrbray.com,2018-05-02:/posts/2018/algorithms-for-random-discrete-structures</id><summary type="html">&lt;p&gt;Many applications require the random sampling of matrices with prescribed structure for modeling, statistical, or aesthetic purposes.  What does it mean for a random variable to be matrix-valued?  What can we say about the eigenvalues of a random matrix?  How can we design algorithms to sample from a target distribution on a group or manifold?  More generally, what can we say deterministic algorithms with random inputs?  Our study of random matrices will lead us to the &lt;em&gt;subgroup algorithm&lt;/em&gt; (Diaconis 1987), which subsumes many familiar random sampling procedures.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Randomness[ref]footnote[/ref] plays a critical role in computer science and applied mathematics.  In the sciences, randomness allows researchers to study average-case behavior of physical models which would otherwise be too complex or time-consuming to simulate exactly.  In computing, randomized algorithms have become essential tools for approximating NP-Hard problems.&lt;/p&gt;
&lt;p&gt;In this post, we will survey strategies for designing algorithms to randomly generate objects with nontrivial structure from a prescribed distribution.  For example, we may wish to choose elements uniformly at random from a group or generate a set of random unitary matrices whose behavior is somehow representative of the class of unitary matrices generally.  The samples are intended for use as input to a known model or algorithm.&lt;/p&gt;
&lt;p&gt;We must take care to distinguish between the three different types of random sampling that are commonly employed.  Genuine &lt;em&gt;uniformly random&lt;/em&gt; samples are easiest to generate, due to independence, but individual realizations will naturally have regions of low and high density.  Our intuitive understanding of uniformity is more like &lt;em&gt;uniformly spaced&lt;/em&gt; samples, where the goal is to maximize coverage and avoid redundancy.  In the extreme, we may want to sample &lt;em&gt;rare events&lt;/em&gt; to study the worst-case behavior of a system.  These differences are shown informally in Figure \ref{fig:kinds-of-randomness}.&lt;/p&gt;
&lt;p&gt;It also helps to keep the reverse problem in mind:  how does a deterministic algorithm behave when fed random inputs?&lt;/p&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;p&gt;In this section, we briefly review the necessary mathematical background and introduce notation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unitary Matrices.&lt;/strong&gt;  A matrix &lt;span class="math"&gt;\(U \in \mathbb{C}^{n \times n}\)&lt;/span&gt; with orthonormal columns is called &lt;em&gt;unitary&lt;/em&gt;.  Equivalently, &lt;span class="math"&gt;\(U U^* = I\)&lt;/span&gt;, where &lt;span class="math"&gt;\(U^*\)&lt;/span&gt; denotes the conjugate transpose.  The eigenvalues of a unitary matrix have unit modulus, and accordingly &lt;span class="math"&gt;\(|\det U| = 1\)&lt;/span&gt;.  Real matrices &lt;span class="math"&gt;\(M \in \mathbb{R}^{n \times n}\)&lt;/span&gt; satisfying these conditions are called &lt;em&gt;orthogonal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Groups.&lt;/strong&gt;  A group &lt;span class="math"&gt;\((G,*)\)&lt;/span&gt; consists of an associative operation &lt;span class="math"&gt;\(* : G \times G \rightarrow G\)&lt;/span&gt; over a set &lt;span class="math"&gt;\(G\)&lt;/span&gt; which contains a (unique) identity element &lt;span class="math"&gt;\(1_G\)&lt;/span&gt; and a (unique) inverse &lt;span class="math"&gt;\(a^{-1}\)&lt;/span&gt; for each element &lt;span class="math"&gt;\(a \in G\)&lt;/span&gt;, with &lt;span class="math"&gt;\(a^{-1} a = a a^{-1} = 1_G\)&lt;/span&gt;.  For convenience, we call &lt;span class="math"&gt;\(a * b\)&lt;/span&gt; multiplication and occasionally write &lt;span class="math"&gt;\(ab\)&lt;/span&gt; instead.  Additive notation &lt;span class="math"&gt;\(a +b\)&lt;/span&gt; is used for commutative groups.  A set &lt;span class="math"&gt;\(S \subset G\)&lt;/span&gt; is a &lt;em&gt;subgroup&lt;/em&gt; if it is closed under multiplication and inverses with respect to the group operation. &lt;/p&gt;
&lt;p&gt;Familiar commutative groups are &lt;span class="math"&gt;\((\mathbb{Z}, +)\)&lt;/span&gt;, &lt;span class="math"&gt;\((\mathbb{Q}, +)\)&lt;/span&gt;, and &lt;span class="math"&gt;\((\mathbb{R}_{\neq 0}, \cdot)\)&lt;/span&gt;.  The invertible matrices over a field &lt;span class="math"&gt;\(F\)&lt;/span&gt; form a non-commutative group &lt;span class="math"&gt;\(\mathrm{GL}_n(F)\)&lt;/span&gt; with respect to matrix multiplication, known as the &lt;em&gt;general linear group&lt;/em&gt;.  The orthogonal matrices &lt;span class="math"&gt;\(\mathrm{O}_n(\mathbb{R})\)&lt;/span&gt; and unitary matrices &lt;span class="math"&gt;\(\mathrm{U}_n(\mathbb{C})\)&lt;/span&gt; both form subgroups of &lt;span class="math"&gt;\(\mathrm{GL}_n(\mathbb{C})\)&lt;/span&gt;. The &lt;em&gt;special linear group&lt;/em&gt; &lt;span class="math"&gt;\(\mathrm{SL}_n(\mathbb{C})\)&lt;/span&gt; is the set of matrices with determinant one.&lt;/p&gt;
&lt;p&gt;Groups of transformations are useful for understanding the &lt;em&gt;invariants&lt;/em&gt; of an object.  The set of all permutations on &lt;span class="math"&gt;\(n\)&lt;/span&gt; elements forms the &lt;em&gt;symmetric group&lt;/em&gt; &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; with respect to function composition.  An important subgroup is the &lt;em&gt;alternating group&lt;/em&gt; &lt;span class="math"&gt;\(A_n\)&lt;/span&gt; consisting of only even permutations.  For a full treatment of group theory, see (Pinter 2010).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Topological Groups.&lt;/strong&gt; Of particular interest are groups whose underlying set possesses additional topological or geometric structure.  Studying transformations on such spaces, e.g. homotopy groups in algebraic topology, can reveal important invariant structure which does not readily present itself through other methods.  In this report, we favor intuition at the expense of rigor, so be warned that statements below may require slight modifications to be technically correct.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Lie group&lt;/em&gt; is a smooth manifold &lt;span class="math"&gt;\(G\)&lt;/span&gt; with group structure such that the multiplication and inverse maps are smooth. A &lt;em&gt;Haar measure&lt;/em&gt; on a compact topological group &lt;span class="math"&gt;\(G\)&lt;/span&gt; is a Borel measure (defined on a &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;-algebra containing the open sets) which is invariant under the group operation, that is, &lt;span class="math"&gt;\(\mu(gE) = \mu(E)\)&lt;/span&gt; for any &lt;span class="math"&gt;\(g \in G\)&lt;/span&gt; and measurable &lt;span class="math"&gt;\(E \subset G\)&lt;/span&gt;.  We sometimes describe this as "translation" or "rotation" invariance depending on the context.  Every compact Lie group has a unique Haar measure &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, which can easily be normalized to a probability measure since &lt;span class="math"&gt;\(\mu(G) &amp;lt; +\infty\)&lt;/span&gt;.  For a proof, see (Tao 2011).  The general linear group &lt;span class="math"&gt;\(GL_n(\mathbb{C})\)&lt;/span&gt; is not compact, but the groups &lt;span class="math"&gt;\(\mathrm{O}_n(\mathbb{R})\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathrm{U}_n(\mathbb{C})\)&lt;/span&gt;, and &lt;span class="math"&gt;\(SL_n(\mathbb{C})\)&lt;/span&gt; are compact.  For this reason, they are sometimes called the &lt;em&gt;classical compact groups&lt;/em&gt;.  Our goal is to generate random samples from these groups according to the Haar measure, which can be thought of as a generalization of the uniform distribution.&lt;/p&gt;
&lt;h1&gt;Random Matrix Theory&lt;/h1&gt;
&lt;p&gt;A &lt;em&gt;matrix ensemble&lt;/em&gt; is a family of matrices together with a probability distribution.  The most well-studied examples are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Gaussian Orthogonal Ensemble&lt;/em&gt;, with i.i.d. real entries &lt;span class="math"&gt;\(\mathcal{N}(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Gaussian Unitary Ensemble&lt;/em&gt;, with i.i.d. complex entries &lt;span class="math"&gt;\(\mathcal{N}(0,1) + i \mathcal{N}(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Gaussian Symplectic Ensemble&lt;/em&gt;, with i.i.d. quaternion entries &lt;span class="math"&gt;\(\mathcal{N} + i \mathcal{N} + j \mathcal{N} + k \mathcal{N}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that these processes do not necessarily produce orthogonal or unitary matrices; the name comes from the orthogonal / unitary invariance of the corresponding probability measures, as in the following lemma:&lt;/p&gt;
&lt;!-- Lemma --&gt;

&lt;div class="theorem"&gt;
&lt;span class="label"&gt;Lemma.&lt;/span&gt; (c.f. Mezzadri 2006)
The measure $d\mu_G$ of the Gaussian Unitary Ensemble is invariant under left and right multiplication of $Z$ by arbitrary unitary matrices,
    $$
    d\mu_G(UZ) = d\mu_G(ZV) = d\mu_G(Z) \text{ for all } U,V \in \mathrm{U}_n(\mathbb{C})
    $$
&lt;/div&gt;

&lt;h2&gt;Eigenvalues and The Circular Law&lt;/h2&gt;
&lt;p&gt;The eigenvalues of a random matrix are themselves random, and it is natural to ask with what distribution.  One might suspect that the particular choice of distribution for the matrix entries would determine the asymptotic behavior; however, the &lt;em&gt;universality principle&lt;/em&gt; (Tao 2010) states that the choice does not matter in the limit, effectively absolving us of our guilt for not choosing a more complicated distribution than the standard normal.  The eigenvalue distribution of matrices with independent Gaussian entries has long been known to obey the &lt;em&gt;circular law&lt;/em&gt;, and universality immediately gives the following theorem.  A related &lt;em&gt;semicircular law&lt;/em&gt; is known for random Hermitian matrices. &lt;/p&gt;
&lt;!-- Theorem: Circular Law --&gt;

&lt;div class="theorem"&gt;
&lt;span class="label"&gt;Theorem.&lt;/span&gt; (Tao 2008) Let $x$ be a complex random variable of mean zero and unit variance.  Let $M_n \in \mathbb{C}^{n \times n}$ be a random matrix whose entries are i.i.d. copies of $x$.  In the limit $n \rightarrow \infty$, the empirical distribution of the eigenvalues of $\frac{1}{\sigma \sqrt{n}} M_n$ converges (both in probability almost surely) to the uniform distribution over the complex unit disk.
&lt;/div&gt;

&lt;p&gt;The circular law is verified empirically in Figure \ref{fig:goe-gue-eigs} for the Gaussian ensembles.  For the real case on the right, there is an unexpected concentration of eigenvalues exactly on the real line, suggesting that the eigenvalue distribution is &lt;em&gt;not absolutely continuous&lt;/em&gt; with respect to Lebesgue measure.  The joint eigenvalue density for the GOE was worked out explicitly by (Edelman 1997) and integrated, leading to the following computation:&lt;/p&gt;
&lt;!-- Theorem:  Probability that GOE has real eigenvalues --&gt;

&lt;div class="theorem"&gt;
&lt;span class="label"&gt;Theorem.&lt;/span&gt; (Edelman 1997)
Let $M \in \mathbb{R}^{n \times n}$ have independent standard normal entries.  The probability that $M$ has $k$ eigenvalues has the form $r + s\sqrt{2}$ for some rational $r,s \in \mathbb{Q}$.  In particular, the probability that a random matrix has all real eigenvalues is $1 / 2^{n(n-1)/4}$.
&lt;/div&gt;

&lt;h2&gt;Random Structured Matrices&lt;/h2&gt;
&lt;p&gt;Some applications call for matrices with additional structure.  For example, it may be desirable to generate random diagonal, symmetric, orthogonal, or upper-triangular matrices.  One might think of several clever ways to accomplish this in Python:&lt;/p&gt;
&lt;style&gt;
#clever-table {
    border-spacing: 20px 0;
}
#clever-table td:nth-child(1) {
    text-align: right;
    margin-right: 10px;
}
#clever-table td:nth-child(2) {
    font-family: monospace;
    font-size: 14px;
}
&lt;/style&gt;

&lt;table id="clever-table"style="margin: 0 auto;"&gt;
    &lt;colgroup&gt;
        &lt;col style="width: 50%"&gt;
        &lt;col style="width: 50%"&gt;
    &lt;/colgroup&gt;
    &lt;tr&gt;&lt;td&gt;Real&lt;/td&gt;&lt;td&gt;A = randn(n,n)&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Complex&lt;/td&gt;&lt;td&gt;B = randn(n,n) + 1j * randn(n,n)&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Diagonal&lt;/td&gt;&lt;td&gt;D = diag(diag(A))&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Symmetric&lt;/td&gt;&lt;td&gt;S = A + A.T&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Orthogonal&lt;/td&gt;&lt;td&gt;Q,_ = qr(A)&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Upper-triangular&lt;/td&gt;&lt;td&gt;_,R = qr(A)&lt;td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Lower-triangular&lt;/td&gt;&lt;td&gt;L = chol(A)&lt;td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;These methods certainly produce matrices of the desired type, but for serious applications it is important to understand the resulting distributions.  Furthermore, the general problem of understanding how deterministic algorithms (such as the &lt;code&gt;numpy&lt;/code&gt;} implementation of QR factorization) is useful for algorithm design.  In the next section, we show that the naive method for generating random unitary matrices is biased and suggest a correction.&lt;/p&gt;
&lt;h1&gt;Random Unitary Matrices&lt;/h1&gt;
&lt;p&gt;Assume we wish to generate random unitary matrices according to the Haar measure on &lt;span class="math"&gt;\(U_n(\mathbb{C})\)&lt;/span&gt;.  The eigenvalues &lt;span class="math"&gt;\(e^{i\theta}\)&lt;/span&gt; of a unitary matrix have unit modulus, so we are effectively choosing random arguments &lt;span style="white-space: pre;"&gt;&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;,&lt;/span&gt; which we would like to be uniform on the complex unit circle.  As shown in Figure \ref{fig:random-unitary-histogram}, the naive method &lt;code&gt;U,_ = qr(B)&lt;/code&gt; applied to a random &lt;code&gt;B&lt;/code&gt; with compex Gaussian entries fails to have a uniform eigenvalue distribution.&lt;/p&gt;
&lt;p&gt;As described by (Mezzadri 2006), this sampling bias arises from the fact that the QR factorization is not unique.  If &lt;span class="math"&gt;\(Z \in \mathrm{GL}_n(\mathbb{C})\)&lt;/span&gt; factorizes as &lt;span class="math"&gt;\(Z = QR\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda = \mathrm{diag}(e^{i\theta_1}, \dots, e^{i\theta_n})\)&lt;/span&gt; is any diagonal unitary matrix, then &lt;span class="math"&gt;\(Z = (Q \Lambda) (\Lambda^{-1}R)\)&lt;/span&gt; is also a valid QR factorization.  In effect, the QR factorization is a &lt;em&gt;multi-valued map&lt;/em&gt;
    &lt;/p&gt;
&lt;div class="math"&gt;$$
    \mathrm{QR} : \mathrm{GL}_n(\mathbb{C}) \rightarrow \mathrm{U}_n(\mathbb{C}) \times T_n(\mathbb{C})
    $$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(T_n(\mathbb{C})\)&lt;/span&gt; is the group of invertible upper triangular matrices.  Different QR factorization algorithms choose different principal values, often in an inconsistent way, which is the source of our error.&lt;/p&gt;
&lt;h2&gt;The Corrected Algorithm&lt;/h2&gt;
&lt;p&gt;To ensure that the QR decomposition with random input produces a unitary matrix distributed according to the Haar measure, we must choose a variation of the map above which is not only single valued but also one-to-one.  We need the following lemma.  Let &lt;span class="math"&gt;\(\Lambda_n(\mathbb{C})\)&lt;/span&gt; denote the group of unitary diagonal matrices.&lt;/p&gt;
&lt;!-- Lemma --&gt;

&lt;div class="theorem"&gt;
&lt;span class="label"&gt;Lemma.&lt;/span&gt;
Let $Z \in \mathbb{C}^{n \times n}$ have two valid QR decompositions $Z = Q_1 R_1 = Q_2 R_2$.  Then, there is $\Lambda \in \Lambda_n(\mathbb{C})$ such that $Q_2 = Q_2 \Lambda^{-1}$ and $R_2 = \Lambda R_1$.
&lt;/div&gt;

&lt;p&gt;Consider the quotient group &lt;span class="math"&gt;\(\Gamma_n(\mathbb{C}) = T_n(\mathbb{C}) / \Lambda_n(\mathbb{C})\)&lt;/span&gt;.  Our corrected algorithm will be a one-to-one map
    &lt;/p&gt;
&lt;div class="math"&gt;$$
    \widehat{\mathrm{QR}} : \mathrm{GL}_n(\mathbb{C}) \rightarrow \mathrm{U}_n(\mathbb{C}) \times \Gamma_n(C)
    $$&lt;/div&gt;
&lt;p&gt;
The upper-rectangular matrix returned by the corrected algorithm will be a representative &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; of &lt;span class="math"&gt;\(\Gamma_n(\mathbb{C})\)&lt;/span&gt;.  If we choose the map &lt;span class="math"&gt;\(\widehat{\mathrm{QR}}\)&lt;/span&gt; to be unitarily invariant with respect to &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, that is,
    &lt;/p&gt;
&lt;div class="math"&gt;$$
    Z \mapsto (Q, \gamma) \implies UZ \mapsto (UQ, \gamma) \text{ for all } U \in \mathrm{U}_n(\mathbb{C})
    $$&lt;/div&gt;
&lt;p&gt;
then by Lemma \ref{lemma:gue-invariance}, the algorithm &lt;span class="math"&gt;\(\widehat{\mathrm{QR}}\)&lt;/span&gt; with input from a Gaussian Unitary Ensemble will be distributed according to the Haar measure on &lt;span class="math"&gt;\(U_n(\mathbb{C})\)&lt;/span&gt;.  It can be shown that the unitary invariance property holds when representatives for &lt;span class="math"&gt;\(\Gamma_n(\mathbb{C})\)&lt;/span&gt; are chosen from the set of upper triangular matrices with real, positive entries.  This suggests the following correction to the naive algorithm.  The results are shown on the right in Figure \ref{fig:random-unitary-histogram}.&lt;/p&gt;
&lt;style&gt;
.highlight {
    padding: 10px;
    background-color: #f5f5f5;
    border: 1px solid #ccc;
    margin: 10px 0;
}

.highlight pre {
    margin: 0;
    font-size: 12px;
}
&lt;/style&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;haar_measure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# naive method&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;# correction&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;PH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;absolute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Q&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;PH&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We have described several basic results in random matrix theory and developed an algorithm for sampling unitary matrices in a uniform way.  The corrected algorithm in the previous section is a special case of the remarkably general &lt;em&gt;subgroup algorithm&lt;/em&gt; (Diaconis 1987), in which uniform random samples from a group &lt;span class="math"&gt;\(G_n\)&lt;/span&gt; are built up from successive uniform samples from a chain of subgroups &lt;span class="math"&gt;\(G_1 \subset G_2 \subset \cdots G_n\)&lt;/span&gt;.  The Fisher-Yates shuffle is a well-known realization of the subgroup algorithm.  The algorithm is very general, but the main difficulty is generating samples from the quotient groups &lt;span class="math"&gt;\(G_{k} / G_{k-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For our specific problem of generating random unitary matrices, we were able to prove that the corrected method produces a result distributed according to Haar measure.  When designing sampling procedures for more complicated structures, this may not be possible, and there has been some work on developing statistical tests on manifolds to determine whether the target distribution has been met (Sepheri 2016).&lt;/p&gt;
&lt;p&gt;I am aware that Lie groups are commonly used in motion planning; the valid orientations of a robot or vehicle form a manifold in configuration space, and there is a group of transformations which describe the allowed behavior.  I strongly suspect that the strategies used in this report could be used to randomly sample valid configurations, which could be useful in simulation or gaming applications.&lt;/p&gt;</content></entry></feed>