<!DOCTYPE html>

<html lang="en">
<head>
    <!-- Metadata -->
	<meta charset="utf-8">

    <!-- Title -->
	<title>Benjamin R. Bray</title>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Droid+Sans+Mono%7CDroid+Serif" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i" rel="stylesheet">
	<!-- Site Styles -->
	<link rel="stylesheet" href="https://benrbray.com/theme/css/style.css" type="text/css">
	<link rel="stylesheet" href="https://benrbray.com/theme/css/pygments.css" type="text/css">
	<!-- Viewport Meta Tag for Mobile Site -->
	<!-- https://developer.mozilla.org/en-US/docs/Mozilla/Mobile/Viewport_meta_tag) -->
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- KaTeX -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
	<!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script>
	<!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous"
		onload='renderMathInElement(document.body, {
			delimiters : [
			{left: "$$", right: "$$", display: true},
			{left: "\\(", right: "\\)", display: false},
			{left: "$", right: "$", display: false},
			],
			macros : {
				"\\X" : "\\mathcal{X}",
				"\\L" : "\\mathcal{L}",
				"\\R" : "\\mathbb{R}",
				"\\C" : "\\mathbb{C}",
			}
		});'></script>
<link rel="stylesheet" href="https://benrbray.com/theme/css/article.css" type="text/css">

<!-- Pygments -->
<link rel="stylesheet" href="http://localhost:8000/theme/css/pygments.css" type="text/css">
<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous"
	onload='renderMathInElement(document.body, {
		delimiters : [
		{left: "$$", right: "$$", display: true},
		{left: "\\(", right: "\\)", display: false},
		{left: "$", right: "$", display: false},
		],
		macros : {
			"\\X" : "\\mathcal{X}",
			"\\L" : "\\mathcal{L}",
			"\\R" : "\\mathbb{R}",
			"\\C" : "\\mathbb{C}",
		}
	});'></script><!-- Table of Contents -->
<script>
var headings = [];

function buildTableOfContents(tocElement){
	// get document headings
	var pageContent = document.getElementById("post-body");
	var headingTags = ["h1", "h2", "h3", "h4", "h5", "h6"];
	var headings = getElementsByTagNames(pageContent, headingTags);
	var headingData = [];
	
	// process headings
	for(var k = 0; k < headings.length; k++){
		// get heading data
		var elem = headings[k];
		var title = elem.innerText;
		var id =  (k+1) + "-" + title.toLowerCase().replace(/\W+/g, "-");
		var depth = parseInt(elem.tagName.substr(1));
		
		headingData.push({
			url: "#" + id,
			title: title,
			depth: depth
		})
		
		// set heading ids
		elem.id = id;
	};

	// quit if no headings found
	if(headingData.length < 1){
		return;
	}

	// build table of contents
	var toc = document.createElement("ol");
	tocElement.innerHTML = "";
	tocElement.appendChild(toc);
	tocElement.className = "";

	// queue of <ol> elements (length represents current depth)
	var queue = [toc];

	for(var k = 0; k < headingData.length; k++){
		let heading = headingData[k];
		
		// pop the queue as needed
		while(heading.depth < queue.length){
			let last = queue.pop();
			queue[queue.length-1].appendChild(last);
		}

		// create new <ol> as needed
		while(heading.depth > queue.length){
			let nested = document.createElement("ol");
			queue.push(nested);
		}

		// add <li> for this heading
		let listItem = document.createElement("li");
		let linkElem = document.createElement("a");
		linkElem.textContent = heading.title;
		linkElem.setAttribute("href", heading.url);
		listItem.appendChild(linkElem);
		queue[queue.length-1].appendChild(listItem);
	}

	while(queue.length > 1){
		let last = queue.pop();
		queue[queue.length-1].appendChild(last);
	}

	// when finished, should have queue == [tocElement]
	tocElement.style.opacity = 1;
	tocElement.style.height = toc.offsetHeight + "px";
}

/* GETELEMENTSBYTAGNAMES
 * Returns all children of `parentElement` with any of the provided tags, in
 * document order.
 */
function getElementsByTagNames(parentElement, tagNames){
	// get all tags
	var elements = [];
	tagNames.forEach(function(tag){
		var result = parentElement.getElementsByTagName(tag);
		elements = elements.concat([].slice.call(result));
	});
	
	// sort elements in document order
	return elements.sort(documentOrderComparator);
}

/* DOCUMENTORDERCOMPARATOR
 * Returns +1 if `a` follows `b`, -1 if `a` precedes `b`, or 0 otherwise.
 */
function documentOrderComparator(a, b){
	// identity check
	if(a == b) return 0;
	// compare positions
	var position = a.compareDocumentPosition(b);
	if(position & 20) return -1;
	if(position & 10) return 1;
	return 0;
}
window.onload = function(){
	buildTableOfContents(document.getElementById("toc"));
}
</script>
</head>

<body>

<!-- CONTENT ------------------------------------------------------------------>

<!-- HEADER -->
<div id="header-box">
	<nav id="header">
	<ul id="header-nav">
		<li>
			<a id="header-name" href="https://benrbray.com/">Benjamin R. Bray</a>
		</li>
		<li><a href="/blog">WRITING</a></li>
		<li><a href="/projects">PROJECTS</a></li>
		<li><a href="/resources">RESOURCES</a></li>
		<li><a href="/resume.pdf">RESUME</a></li>
	</ul>
	</nav>
</div>

<!-- PRECONTENT -->
<div id="pre-content">
<!-- Article Banner Image -->
<div class="page-banner">
	<img src="https://benrbray.com/images/thumbnails/deutsche-rundschau_long.png"></img>
	<div class="banner-comment">Image: <a href="https://twitter.com/FontaneArchiv/status/999271012591177728">@FontaneArchiv</a></div>
</div>
</div>

<div id="page">
	
	<!-- CONTENT -->
	<div id="content">

<div class="center">

<!-- Post Header -->
<header class="article-header">
	<h1 class="article-title">Digital Humanities &amp; German&nbsp;Periodicals</h1>
	<div class="article-info">
		<!-- Date -->
		<div class="article-date">
			Posted in projects on December 01, 2016
		</div>
		<!-- Tags / Tools -->
			<nav class="article-tags">
<a href="/tag/nlp">nlp</a><a href="/tag/topic-models">topic-models</a><a href="/tag/machine-learning">machine-learning</a><a href="/tag/web-dev">web-dev</a>					<a class="tool">python</a>
					<a class="tool"> flask</a>
					<a class="tool"> gensim</a>
			</nav>
	</div>
</header>

<!-- Table of Contents (Requires JavaScript) -->
<div id="toc" class="displaynone"></div>

<!-- Post Context -->
<div class="article-content">
	<div id="post-body">
	<h1>Overview</h1>
<p>As an undergraduate research assistant, I spent three years as the primary developer for an NLP-driven web application built to assist a humanities professor (<a href="https://lsa.umich.edu/german/people/faculty/pmcisaac.html">Dr. Peter McIsaac</a>, University of Michigan) with his research on 19th-century German literature.  The application allowed him to run statistical topic models (<a href="http://jmlr.org/papers/volume3/blei03a/blei03a.pdf">LDA</a>, <a href="http://proceedings.mlr.press/v15/wang11a/wang11a.pdf">HDP</a>, <a href="https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf">DTM</a>, etc.) on a large corpus of text and displayed helpful visualizations of the results.  The application was built using <strong>Python</strong> / <strong>Flask</strong> / <strong>Bootstrap</strong> and also supported toponym detection and full-text search.  We used <a href="https://radimrehurek.com/gensim/"><code>gensim</code></a> for topic modeling.</p>
<p>Using the web application I built, my supervisor was able to effectively detect cultural and historical trends in a large corpus of previously unstudied documents<span class="aside">This is a cheeky remark!</span>.  Our efforts led to a number of publications in humanities journals and conferences, including <a href="http://www.jstor.org/stable/10.7722/j.ctt5vj848.11">[McIsaac 2014]</a>:</p>
<blockquote class="citation">
McIsaac, Peter M. <a href="http://www.jstor.org/stable/10.7722/j.ctt5vj848.11">“Rethinking Nonfiction: Distant Reading the Nineteenth-Century Science-Literature Divide.”</a> <i>Distant Readings: Topologies of German Culture in the Long Nineteenth Century</i>, edited by Matt Erlin and Lynne Tatlock, ed., Boydell and Brewer, 2014, pp. 185–208.
</blockquote>

<h2>Motivation</h2>
<p>Our analysis focused on a corpus of widely-circulated periodicals, published in Germany during the 19th-century around the time of the administrative <a href="https://en.wikipedia.org/wiki/Unification_of_Germany">unification</a> of Germany in 1871.  Through <a href="https://www.hathitrust.org/">HathiTrust</a> and partnerships with university libraries, we obtained digital scans of the following periodicals:</p>
<ul>
<li><em>Deutsche Rundschau</em> (1874-1964)</li>
<li><em>Westermann's Illustrirte Monatshefte</em> (1856-1987)</li>
<li><em>Die Gartenlaube</em> (1853-1944)</li>
</ul>
<p>These periodicals, published weekly or monthly, were among Germany's most widely-read print material in the latter half of the nineteenth century, and served as precursors <span class="aside">This is a longer remark that provides more detail about something in the main article.</span>to the modern magazine.  Scholars have long recognized the cultural significance of these publications (c.f. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=yGHo-Alkp84C&amp;oi=fnd&amp;pg=PR9&amp;dq=belgum+2002+popularizing+the+nation+Audience,+Representation,+and+the+Production+of+Identity+in+Die+Gartenlaube&amp;ots=VFwEvxdUUS&amp;sig=kF6W0ktdb6BOcD1TY7Rdwtf_tsc#v=onepage&amp;q&amp;f=false">[Belgum 2002]</a>), but their enormous volume had so far precluded comprehensive study.</p>
<figure>
    <div class="img-gallery horizontal">
        <img src="/images/dhgp/westermanns-cover.jpg">
        <img src="/images/dhgp/gartenlaube.jpg">
    </div>
    <figcaption>Cover of <cite>Westermann's Monatshefte</cite> and front page of <cite>Die Gartenlaube</cite>.  Courtesy of HathiTrust.</figcaption>
</figure>

<p>Using statistical methods, including <a href="http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/">topic models</a>, we aimed to study the development of a German national identity following the 1848 revolutions, through the 1871 unification, and leading up to the world wars of the twentieth century.  This approach is commonly referred to as <strong>digital humanities</strong> or <strong>distant reading</strong> (in contrast to <a href="https://en.wikipedia.org/wiki/Close_reading">close reading</a>).</p>
<h2>Preprocessing</h2>
<p>Initially, we only had access to digital scans of books printed in a difficult-to-read blackletter font.  In order to convert our scanned images to text, I used <a href="https://github.com/tesseract-ocr">Google Tesseract</a> to train a custom optical character recognition (OCR) model specialized to fonts from our corpus.  Tesseract performed quite well, but our scans exhibited a number of characteristics that introduced errors into the OCR process:</p>
<ul>
<li>Poor scan quality (causing speckles, erosion, dilation, etc.)</li>
<li>Orthographic differences from modern German, including ligatures and the <a href="https://en.wikipedia.org/wiki/Long_s">long s</a></li>
<li>Inconsistent layouts (floating images, multiple columns per page, etc.)</li>
<li>Blackletter fonts which are difficult to read, even for humans</li>
<li>The use of fonts such as Antiqua for dates and foreign words</li>
<li>Headers, footers, page numbers, illustrations, and hyphenation</li>
</ul>
<p>The examples below highlight some of the challenges we faced during the OCR phase.</p>
<figure role="group">
    <div class="img-gallery large">
        <figure class="img-col-2" >
            <img src="/images/dhgp/deutsche-rundschau-wikipedia.jpg">
            <figcaption>From <cite>Deutsche Rundschau</cite>, courtesy of <a href="https://www.hathitrust.org/">HathiTrust</a>.</figcaption>
        </figure>
        <figure>
            <img src="/images/dhgp/fraktur-antiqua-wiki.jpg">
            <figcaption>Wikipedia, <a href="https://en.wikipedia.org/wiki/Antiqua%E2%80%93Fraktur_dispute">Antiqua-Fraktur Dispute</a></figcaption>
        </figure>
        <figure>
            <img src="/images/dhgp/gartenlaube-1.jpg">
            <figcaption>From <cite>Die Gartenlaube</cite>, courtesy of <a href="https://www.hathitrust.org/">HathiTrust</a>.</figcaption>
        </figure>
    </div>
</figure>

<p>As a result, significant pre- and post-processing of OCR results was necessary.  We combined a number of approaches in order to reduce the error rate to an acceptable level:</p>
<ul>
<li>I used <a href="https://processing.org/">Processing</a> to remove noise and other scanning artifacts from our images.</li>
<li>I wrote code to automatically remove running headers, text decorations, and page numbers.</li>
<li>Through manual inspection of a small number of documents, we compiled a list of common OCR mistakes.  I developed scripts to automatically propagate these corrections across the entire corpus.</li>
<li>I experimented with several custom OCR-correction schemes to correct as many mistakes as possible and highlight ambiguities.  Our most successful approach used a <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Model</a> to correct sequences of word fragments.  Words were segmented using <a href="https://www.sciencedirect.com/science/article/pii/0020027174900448">Letter Successor Entropy</a>.</li>
</ul>
<p>With these improvements, we found that our digitized texts were good enough for the type of exploratory analysis we had in mind.  By evaluating our OCR pipeline on a synthetic dataset of "fake" scans with known text and a configurable amount of noise (speckles, erosion, dilation, etc.), we found that our OCR correction efforts improved accuracy from around 80% to 95% or higher.</p>
<h2>Topic Modeling</h2>
<p>In natural language processing, <b>topic modeling</b> is a form of statistical analysis used to help index and explore large collections of text documents.  The output of a topic model typically includes:</p>
<ul>
<li>A list of <b>topics</b>, each represened by a list of related words.  Each word may also have an associated weight, indicating how strongly a word relates to this topic.  For example:<ul>
<li>(Topic 1) <i>sport, team, coach, ball, coach, team, race, bat, run, swim...</i></li>
<li>(Topic 2) <i>country, government, official, governor, tax, approve, law...</i></li>
<li>(Topic 3) <i>train, bus, passenger, traffic, bicycle, pedestrian...</i></li>
</ul>
</li>
<li>A <b>topic probability vector</b> for each document, representing the importance of each topic to this document.  For example, a document about the Olympics may be 70% sports, 20% government, and 10% transportation.</li>
</ul>
<p>The most popular topic model is <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation (LDA)</a>, which is succinctly described by the following probabilistic graphical model.  There are <span class="math">\(T\)</span> topics, <span class="math">\(M\)</span> documents, <span class="math">\(N\)</span> words per document, and <span class="math">\(V\)</span> words in the vocabulary.</p>
<figure>
    <div class="img-gallery horizontal" style="align-items: center">
        <!-- Graphical Model -->
        <img src="/images/dhgp/pgm-lda.png">
        <!-- Distributions -->
        <div style="font-size: 0.8em">
        $$\begin{aligned}
        \text{hyperparameters}  &&& \alpha \in \mathbb{R}^{T}, \eta \in \mathbb{R}^{V}\\
        \text{topics}           && \beta_t \mid \eta & \stackrel{iid}{\sim} \mathrm{Dirichlet}(\eta)          \\
        \text{topic mixtures}   && \theta_m \mid \alpha  &\stackrel{iid}{\sim} \mathrm{Dirichlet}(\alpha)           \\
        \text{topic indicators} && z_{mn} \mid \theta_m  &\stackrel{iid}{\sim} \mathrm{Categorical}(\theta_m)       \\
        \text{word indicators}  && w_{mn} \mid z_{mn}    &\stackrel{iid}{\sim} \mathrm{Categorical}(\beta_{z_{mn}})
        \end{aligned}$$
        </div>
    </div>
</figure>

<p>Each topic <span class="math">\(t\)</span> is represented by a probability distribution <span class="math">\(\beta_t\)</span> over the vocabulary, indicating how likely each word is to appear under topic <span class="math">\(t\)</span>.  LDA posits that documents are written using the following <b>generative process</b>:</p>
<ol>
<li>For each document <span class="math">\(d_{m}\)</span>,<ol>
<li>Decide in what proportions <span class="math">\(\theta_m = (\theta_{m1},\dots,\theta_{mt})\)</span> each topic will appear.</li>
<li>To choose each each word <span class="math">\(w_{mn}\)</span>,<ol>
<li>According to <span class="math">\(\theta_m\)</span>, randomly decide which topic to use for this word.</li>
<li>Randomly sample a word according to the chosen topic.</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Of course, this is not how humans actually write.  LDA represents documents as <b>bags-of-words</b>, ignoring word order and sentence structure.  When topic models are used to index or explore large corpora, as was our goal, this is an acceptable compromise.  Given a collection of documents, LDA attempts to "invert" the generative process by computing a <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a> of the topics <span class="math">\(\beta_t\)</span> and topic mixtures <span class="math">\(\theta_m\)</span>.  These estimates are typically computed using <b>variational expectation-maximziation</b>.</p>
<h2>DHGP Browser</h2>
<p>Using <strong>Python / Flask / Bootstrap</strong>, I built a web application enabling humanities researchers to train, visualize, and save topic models.  Features:</p>
<ul>
<li>Support for several popular topic models:<ul>
<li>Online Latent Dirichlet Allocation (via <code>gensim</code>)</li>
<li>Online Hierarchical Dirichlet Process (via <code>gensim</code>)</li>
<li>Dynamic Topic Models (custom implementation based <a href="https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf">[Blei 2006]</a>)</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Toponym_resolution">Toponym Resolution</a> for identifying and mapping place names mentioned in our texts </li>
<li>Full-text / metadata search using <a href="www.elastic.co">ElasticSearch</a></li>
<li>Support for any corpus with metadata saved in JSON format.</li>
</ul>
<p>I no longer have access to the most up-to-date version of <code>dhgp-browser</code>, but here are some screenshots from mid-2015:</p>
<figure>
    <div class="img-gallery col-2">
        <figure>
            <img src="/images/dhgp/dhgp-browser-3.png">
            <figcaption>Topic View</figcaption>
        </figure>
        <figure>
            <img src="/images/dhgp/dhgp-browser-2.png">
            <figcaption>Topic Listing</figcaption>
        </figure>
        <figure>
            <img src="/images/dhgp/dhgp-browser-1.png">
            <figcaption>Corpus View</figcaption>
        </figure>
        <figure>
            <img src="/images/dhgp/dhgp-browser-4.png">
            <figcaption>Document Graph</figcaption>
        </figure>
    </div>
</figure>

<h1>Miscellaneous</h1>
<h2>UROP Symposium Poster</h2>
<p>The poster below summarizes the progress made during my first year on the project, which I initially joined through the <a href="https://lsa.umich.edu/urop">UROP Program</a> at UM.  After my first year, I was hired to continue working on the project as an undergraduate research assistant.</p>
<p><a href="/static/dhgp/dhgp_urop-poster_benrbray.pdf">
<img src="/images/dhgp/dhgp-poster.png"></img>
</a></p>
<h1>References</h1>
<ul>
<li><strong>[McIsaac 2014]</strong> McIsaac, Peter M. <a href="http://www.jstor.org/stable/10.7722/j.ctt5vj848.11">“Rethinking Nonfiction: Distant Reading the Nineteenth-Century Science-Literature Divide.”</a> <em>Distant Readings: Topologies of German Culture in the Long Nineteenth Century</em>, edited by Matt Erlin and Lynne Tatlock, ed., Boydell and Brewer, 2014, pp. 185–208.</li>
<li><strong>[Belgum 2002]</strong> Belgum, Kirsten. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=yGHo-Alkp84C&amp;oi=fnd&amp;pg=PR9&amp;dq=belgum+2002+popularizing+the+nation+Audience,+Representation,+and+the+Production+of+Identity+in+Die+Gartenlaube&amp;ots=VFwEvxdUUS&amp;sig=kF6W0ktdb6BOcD1TY7Rdwtf_tsc#v=onepage&amp;q&amp;f=false">Popularizing the Nation: Audience, Representation, and the Production of Identity in Die Gartenlaube</a>, 1853-1900. U of Nebraska Press, 1998.</li>
</ul>
	</div>
</div>

</div>
	</div>

	<!-- Page Footer -->
	<div id="page-footer">
		<span style="float: left; vertical-align: middle">
			<img id="visitor-count" src="https://hitcounter.pythonanywhere.com/count/tag.svg" alt="Hits">
			Powered by <a href="http://blog.getpelican.com/">Pelican</a> and hosted on <a href="https://github.com/benrbray/benrbray.github.io-source">GitHub</a>.
		</span>
		<span style="float: right">
			Last updated on October 06, 2020
		</span>
	</div>
</div>

</body>
</html>